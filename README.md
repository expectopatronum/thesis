# thesis

This web page accompanies the thesis written by Verena Praher ``Human-friendly Explanations of Deep Models in Music Information Retrieval''. Some of the publications provided listening examples spread across different web sites. Here I collected all those examples, sorted by chapter.

## Listenable Representations (Chapter 3)

### Demonstration on Real-world Data: Automatic Music Tagging

* [audioLIME: Listenable Explanations Using Source Separation (MML2020)](https://soundcloud.com/veroamilbe/sets/mml2020-explanation-example)
* [Towards Musically Meaningful Explanations Using Source Separation](https://expectopatronum.github.io/thesis/chapter3/towards_musically_meaningful_explanations.html)


### Demonstration on Real-world Data: Music Recommendation

* [LEMONS: Listenable Explanations for Music recOmmeNder Systems](https://github.com/cpjku/lemons)

## Mid-level Interpretable Representations (Chapter 4)

* [Towards Explainable Music Emotion Recognition: The Route via Mid-level Features (ISMIR 2019)](https://expectopatronum.github.io/thesis/chapter4/ismir_example.html)
* [Two-level Explanations for Music Emotion Recognition (ML4MD @ ICML 2019)](https://expectopatronum.github.io/thesis/chapter4/ICML_example.html)

## Concept-based Explanations (Chapter 5)

* [ISMIR 2022](https://github.com/CPJKU/composer_concept)


## Technical Details

If you want to reproduce figures (from this repository, which only contains additional figures; for other figures see the corresponding publications and repositories) install the following libaries (ran with Python 3.12.0):

- seaborn=0.13.2
- scikit-image=0.25.0
- librosa=0.10.2.post1
<!-- - Cython=3.0.11
- madmom=0.16.1 -->

